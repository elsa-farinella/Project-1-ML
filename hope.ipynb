{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data_2 import *\n",
    "from data_imputation import *\n",
    "import pandas as pd\n",
    "from implementations import *\n",
    "from additional_methods import *\n",
    "from helpers import *\n",
    "import numpy as np\n",
    "\n",
    "# AUTORELOAD\n",
    "%load_ext autoreload   \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAS USING SELF IMPUTED DATA\n",
    "\n",
    "path_dataset = \"/Users/robinfaro/Desktop/ML_EPFL/Project1/dataset_to_release/x_train.csv\"\n",
    "path_labels = \"/Users/robinfaro/Desktop/ML_EPFL/Project1/dataset_to_release/y_train.csv\"\n",
    "data = np.genfromtxt(path_dataset,\n",
    "                    delimiter=\",\",\n",
    "                    skip_header=1,\n",
    "                    usecols=[27, 31, 33, 38, 39, 40, 49, 51, 53, 61, 70, 88, 261, 266, 277, 278, 28, 29, 34, 35, 41, 43, 44, 45, 46, 47, 48,\n",
    "                             66, 137, 145, 248]) #1, 32, 255, 307, 67, 68, 69, 71, 72]) \n",
    "                    #27, 31, 33, 38, 39, 40, 49, 51, 53, 61, 70, 88, 261, 266, 277, 278\n",
    "                    #27, 35, 40, 49, 250, 28, 261, 266, 306, 259\n",
    "\n",
    "y = np.genfromtxt(\n",
    "    path_labels,\n",
    "    delimiter=\",\",\n",
    "    skip_header=1,\n",
    "    usecols=[1]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use imputed data\n",
    "path_dataset = \"/Users/robinfaro/Desktop/ML_EPFL/Project1/train_data_imputed.csv\"\n",
    "path_labels = \"/Users/robinfaro/Desktop/ML_EPFL/Project1/dataset_to_release/y_train.csv\"\n",
    "\n",
    "data = np.genfromtxt(path_dataset, delimiter=\",\")\n",
    "\n",
    "y = np.genfromtxt(\n",
    "    path_labels,\n",
    "    delimiter=\",\",\n",
    "    skip_header=1,\n",
    "    usecols=[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change -1 labels to 0\n",
    "y[y== -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(data):\n",
    "    \"\"\"\n",
    "    This function returns the mode of a column, even for non-integer data.\n",
    "    \"\"\"\n",
    "    unique_elements, counts = np.unique(data, return_counts=True)\n",
    "    max_count_index = np.argmax(counts)\n",
    "    mode = unique_elements[max_count_index]\n",
    "    return mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPROCESSING\n",
    "data[np.isnan(data)] = -1\n",
    "\n",
    "data[:, 0][data[:, 0] > 5] = -1\n",
    "data[:, 0][data[:, 0] >= 0] /= 5\n",
    "data[:, 0][data[:, 0] == -1] = np.mean(data[:, 0][data[:, 0] != -1])\n",
    "\n",
    "data[:, 1][data[:, 1] > 2] = -1\n",
    "data[:, 1][data[:, 1] == 2] = 0\n",
    "data[:, 0][data[:, 0] == -1] = mode(data[:, 0][data[:, 0] != -1])\n",
    "\n",
    "data[:, 2][data[:, 2] > 2] = -1\n",
    "data[:, 2][data[:, 2] == 2] = 0\n",
    "data[:, 2][data[:, 2] == -1] = mode(data[:, 2][data[:, 2] != -1])\n",
    "\n",
    "data[:, 3][data[:, 3] > 4] = -1\n",
    "data[:, 3][data[:, 3] >= 0] /= 4\n",
    "data[:, 3][data[:, 3] == -1] = np.mean(data[:, 3][data[:, 3] != -1])\n",
    "\n",
    "data[:, 4][data[:, 4] > 2] = -1\n",
    "data[:, 4][data[:, 4] == 2] = 0\n",
    "data[:, 4][data[:, 4] == -1] = mode(data[:, 4][data[:, 4] != -1])\n",
    "\n",
    "data[:, 5][data[:, 5] > 2] = -1\n",
    "data[:, 5][data[:, 5] == 2] = 0\n",
    "data[:, 5][data[:, 5] == -1] = mode(data[:, 5][data[:, 5] != -1])\n",
    "\n",
    "data[:, 6][data[:, 6] > 4] = -1\n",
    "data[:, 6][data[:, 6] == 2] = 0\n",
    "data[:, 6][data[:, 6] == 3] = 0\n",
    "data[:, 6][data[:, 6] == 4] = 2\n",
    "data[:, 6][data[:, 6] >= 0] /= 2\n",
    "data[:, 6][data[:, 6] == -1] = mode(data[:, 6][data[:, 6] != -1])\n",
    "\n",
    "data[:, 7][data[:, 7] == 2] = 0\n",
    "\n",
    "data[:, 8][data[:, 8] > 6] = -1\n",
    "data[:, 8][data[:, 8] >= 0] /= 6\n",
    "data[:, 8][data[:, 8] == -1] = np.mean(data[:, 8][data[:, 8] != -1])\n",
    "\n",
    "data[:, 9][data[:, 9] > 8] = -1\n",
    "data[:, 9][data[:, 9] >= 0] /= 8\n",
    "data[:, 9][data[:, 9] == -1] = np.mean(data[:, 9][data[:, 9] != -1])\n",
    "\n",
    "data[:, 10][data[:, 10] > 2] = -1\n",
    "data[:, 10][data[:, 10] == 2] = 0\n",
    "data[:, 10][data[:, 10] == -1] = mode(data[:, 10][data[:, 10] != -1])\n",
    "\n",
    "data[:, 11][data[:, 11] > 2] = -1\n",
    "data[:, 11][data[:, 11] == 2] = 0\n",
    "data[:, 11][data[:, 11] == -1] = mode(data[:, 11][data[:, 11] != -1])\n",
    "\n",
    "data[:, 12][data[:, 12] > 2] = -1\n",
    "data[:, 12][data[:, 12] == 1] = 0\n",
    "data[:, 12][data[:, 12] == 2] = 1\n",
    "data[:, 12][data[:, 12] == -1] = mode(data[:, 12][data[:, 12] != -1])\n",
    "\n",
    "data[:, 13][data[:, 13] > 2] = -1\n",
    "data[:, 13][data[:, 13] == 1] = 0\n",
    "data[:, 13][data[:, 13] == 2] = 1\n",
    "data[:, 13][data[:, 13] == -1] = mode(data[:, 13][data[:, 13] != -1])\n",
    "\n",
    "data[:, 14][data[:, 14] > 10] = 10\n",
    "data[:, 14][data[:, 14] >= 0] /= 10\n",
    "data[:, 14][data[:, 14] == -1] = np.mean(data[:, 14][data[:, 14] != -1])\n",
    "\n",
    "data[:, 15][data[:, 15] > 10] = 10\n",
    "data[:, 15][data[:, 15] >= 0] /= 10\n",
    "data[:, 15][data[:, 15] == -1] = np.mean(data[:, 15][data[:, 15] != -1])\n",
    "\n",
    "data[:, 16:18][data[:, 16:18] > 30] = -1\n",
    "data[:, 16:18][data[:, 16:18] >= 0] /= 30\n",
    "data[:, 16:18][data[:, 16:18] == -1] = np.mean(data[:, 16:18][data[:, 16:18] != -1])\n",
    "\n",
    "data[:, 18][data[:, 18] > 4] = -1\n",
    "data[:, 18][data[:, 18] >= 0] /= 4\n",
    "data[:, 18][data[:, 18] == -1] = mode(data[:, 18][data[:, 18] != -1])\n",
    "\n",
    "data[:, 19][data[:, 19] > 4] = -1\n",
    "data[:, 19][data[:, 19] == 4] = 1\n",
    "data[:, 19][data[:, 19] >= 2] = 0\n",
    "data[:, 19][data[:, 19] == -1] = mode(data[:, 19][data[:, 19] != -1])\n",
    "\n",
    "data[:, 20][data[:, 20] > 2] = -1\n",
    "data[:, 20][data[:, 20] == 2] = 0\n",
    "data[:, 20][data[:, 20] == -1] = mode(data[:, 20][data[:, 20] != -1])\n",
    "\n",
    "data[:, 21][data[:, 21] > 2] = -1\n",
    "data[:, 21][data[:, 21] == 2] = 0\n",
    "data[:, 21][data[:, 21] == -1] = mode(data[:, 21][data[:, 21] != -1])\n",
    "\n",
    "data[:, 22][data[:, 22] > 2] = -1\n",
    "data[:, 22][data[:, 22] == 2] = 0\n",
    "data[:, 22][data[:, 22] == -1] = mode(data[:, 22][data[:, 22] != -1])\n",
    "\n",
    "data[:, 23][data[:, 23] > 2] = -1\n",
    "data[:, 23][data[:, 23] == 2] = 0\n",
    "data[:, 23][data[:, 23] == -1] = mode(data[:, 23][data[:, 23] != -1])\n",
    "\n",
    "data[:, 24][data[:, 24] > 2] = -1\n",
    "data[:, 24][data[:, 24] == 2] = 0\n",
    "data[:, 24][data[:, 24] == -1] = mode(data[:, 24][data[:, 24] != -1])\n",
    "\n",
    "data[:, 25][data[:, 25] > 2] = -1\n",
    "data[:, 25][data[:, 25] == 2] = 0\n",
    "data[:, 25][data[:, 25] == -1] = mode(data[:, 25][data[:, 25] != -1])\n",
    "\n",
    "data[:, 26][data[:, 26] > 2] = -1\n",
    "data[:, 26][data[:, 26] == 2] = 0\n",
    "data[:, 26][data[:, 26] == -1] = mode(data[:, 26][data[:, 26] != -1])\n",
    "\n",
    "data[:, 27][data[:, 27] > 2] = -1\n",
    "data[:, 27][data[:, 27] == 2] = 0\n",
    "data[:, 27][data[:, 27] == -1] = mode(data[:, 27][data[:, 27] != -1])\n",
    "\n",
    "data[:, 28][data[:, 28] > 2] = -1\n",
    "data[:, 28][data[:, 28] == 2] = 0\n",
    "data[:, 28][data[:, 28] == -1] = mode(data[:, 28][data[:, 28] != -1])\n",
    "\n",
    "data[:, 29][data[:, 29] > 2] = -1\n",
    "data[:, 29][data[:, 29] == 2] = 0\n",
    "data[:, 29][data[:, 29] == -1] = mode(data[:, 29][data[:, 29] != -1])\n",
    "\n",
    "data[:, 30][data[:, 30] > 2] = -1\n",
    "data[:, 30][data[:, 30] == 2] = 0\n",
    "data[:, 30][data[:, 30] == -1] = mode(data[:, 30][data[:, 30] != -1])\n",
    "\n",
    "\n",
    "# #### ADDED COLUMNS ####\n",
    "\n",
    "# data[:, 31][data[:, 31] > 72] = -1\n",
    "# data[:, 31][data[:, 31] >= 0] /= 72\n",
    "# data[:, 31][data[:, 31] == -1] = np.mean(data[:, 31][data[:, 31] != -1])\n",
    "\n",
    "# data[:, 32][data[:, 32] > 3] = -1\n",
    "# data[:, 32][data[:, 32] == 3] = 0\n",
    "# data[:, 32][data[:, 32] == 2] = 1 \n",
    "# data[:, 32][data[:, 32] >= 0] /= 3\n",
    "\n",
    "# data[:, 33][data[:, 33] > 4] = -1\n",
    "# data[:, 33][data[:, 33] >= 0] /= 4\n",
    "# data[:, 33][data[:, 33] == -1] = np.mean(data[:, 33][data[:, 33] != -1])\n",
    "\n",
    "# data[:, 34][data[:, 34] > 2] = -1\n",
    "# data[:, 34][data[:, 34] == 2] = 0\n",
    "# data[:, 34][data[:, 34] == -1] = mode(data[:, 34][data[:, 34] != -1])\n",
    "\n",
    "# data[:, 35][data[:, 35] > 2] = -1\n",
    "# data[:, 35][data[:, 35] == 2] = 0\n",
    "# data[:, 35][data[:, 35] == -1] = mode(data[:, 35][data[:, 35] != -1])\n",
    "\n",
    "# data[:, 36][data[:, 36] > 2] = -1\n",
    "# data[:, 36][data[:, 36] == 2] = 0\n",
    "# data[:, 36][data[:, 36] == -1] = mode(data[:, 36][data[:, 36] != -1])\n",
    "\n",
    "# data[:, 37][data[:, 37] > 2] = -1\n",
    "# data[:, 37][data[:, 37] == 2] = 0\n",
    "# data[:, 37][data[:, 37] == -1] = mode(data[:, 37][data[:, 37] != -1])\n",
    "\n",
    "# data[:, 38][data[:, 38] > 2] = -1\n",
    "# data[:, 38][data[:, 38] == 2] = 0\n",
    "# data[:, 38][data[:, 38] == -1] = mode(data[:, 38][data[:, 38] != -1])\n",
    "\n",
    "# data[:, 39][data[:, 39] > 2] = -1\n",
    "# data[:, 39][data[:, 39] == 2] = 0\n",
    "# data[:, 39][data[:, 39] == -1] = mode(data[:, 39][data[:, 39] != -1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = split_data(data, y, 0.95, seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = val_data[:, 0:-1]\n",
    "val_y = val_data[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop data on train set\n",
    "\n",
    "keep_percentage = 0.36\n",
    "\n",
    "np.random.seed(13)\n",
    "\n",
    "# compute the number of rows to keep\n",
    "zero_rows = train_data[train_data[:, -1] == -0]\n",
    "non_zero_rows = train_data[train_data[:, -1] != -0]\n",
    "num_rows_to_keep = int(len(zero_rows) * keep_percentage)\n",
    "\n",
    "# Select randomly the rows to keep\n",
    "selected_rows = np.random.choice(zero_rows.shape[0], num_rows_to_keep, replace=False)\n",
    "\n",
    "# Join the selected rows with the non-zero rows\n",
    "filtered_data = zero_rows[selected_rows]\n",
    "filtered_data = np.concatenate((filtered_data, non_zero_rows), axis=0)\n",
    "\n",
    "train_x = filtered_data[:, :-1]\n",
    "train_y = filtered_data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(val_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "train_tx = build_model_data(train_x)\n",
    "val_tx = build_model_data(val_x)\n",
    "max_iters = 1000\n",
    "gamma = 2\n",
    "initial_w = np.full(train_tx.shape[1], -1)\n",
    "#initial_w = checkpoint1\n",
    "w, loss = logistic_regression(train_y, train_tx, initial_w, max_iters, gamma, False)\n",
    "f1 = compute_f1_logistic(val_y, val_tx, w)\n",
    "print(\"F1 score: \", f1)\n",
    "confusion_matrix(val_y, val_tx, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint1 = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    \"\"\"Normalize the original data set.\"\"\"\n",
    "    # Calcola la media e la deviazione standard\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "\n",
    "    # Aggiungi una piccola costante per evitare divisioni per zero\n",
    "    epsilon = 1e-8\n",
    "    std[std < epsilon] = epsilon\n",
    "\n",
    "    # Normalizza il data set\n",
    "    data = (data - mean) / std\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_normalized = normalize_data(train_x)\n",
    "val_x_normalized = normalize_data(val_x)\n",
    "\n",
    "# logistic regression\n",
    "train_tx = build_model_data(train_x_normalized)\n",
    "val_tx = build_model_data(val_x_normalized)\n",
    "max_iters = 1000\n",
    "gamma = 1\n",
    "initial_w = np.full(train_tx.shape[1], -1)\n",
    "#initial_w = checkpoint2\n",
    "w, loss = logistic_regression(train_y, train_tx, initial_w, max_iters, gamma, False)\n",
    "f1 = compute_f1_logistic(val_y, val_tx, w)\n",
    "print(\"F1 score: \", f1)\n",
    "confusion_matrix(val_y, val_tx, w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint2 = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = np.arange(0.2, 0.5 , 0.01)\n",
    "\n",
    "F1S = np.zeros(len(percentages))\n",
    "\n",
    "# logistic regression   \n",
    "max_iters = 1000\n",
    "gamma = 2\n",
    "initial_w = np.full(train_tx.shape[1], -1)\n",
    "#initial_w = checkpoint2\n",
    "for i, percentage in enumerate(percentages):\n",
    "    keep_percentage = percentage\n",
    "    np.random.seed(13)\n",
    "    # compute the number of rows to keep\n",
    "    zero_rows = train_data[train_data[:, -1] == -0]\n",
    "    non_zero_rows = train_data[train_data[:, -1] != -0]\n",
    "    num_rows_to_keep = int(len(zero_rows) * keep_percentage)\n",
    "    # Select randomly the rows to keep\n",
    "    selected_rows = np.random.choice(zero_rows.shape[0], num_rows_to_keep, replace=False)\n",
    "    # Join the selected rows with the non-zero rows\n",
    "    filtered_data = zero_rows[selected_rows]\n",
    "    filtered_data = np.concatenate((filtered_data, non_zero_rows), axis=0)\n",
    "    train_x = filtered_data[:, :-1]\n",
    "    train_y = filtered_data[:, -1]\n",
    "    print(\"percentage: \", percentage)\n",
    "    train_x_normalized = normalize_data(train_x)\n",
    "    val_x_normalized = normalize_data(val_x)\n",
    "    train_tx = build_model_data(train_x_normalized)\n",
    "    val_tx = build_model_data(val_x_normalized)\n",
    "    w, loss = logistic_regression(train_y, train_tx, initial_w, max_iters, gamma, False)\n",
    "    f1 = compute_f1_logistic(val_y, val_tx, w)\n",
    "    print(\"F1 score: \", f1)\n",
    "    F1S[i] = f1\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F1S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot f1 scores vs percentages\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(percentages, F1S)\n",
    "plt.xlabel(\"Percentage\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.title(\"F1 score vs percentage of rows with negative label kept\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "train_tx = build_model_data(train_x)\n",
    "val_tx = build_model_data(val_x)\n",
    "max_iters = 1000\n",
    "gamma = 0.1\n",
    "initial_w = np.full(train_tx.shape[1], -1)\n",
    "#initial_w = checkpoint1\n",
    "w, loss = reg_logistic_regression(train_y, train_tx, 0.1, initial_w, max_iters, gamma)\n",
    "f1 = compute_f1(val_y, val_tx, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.34763572679509636\n",
      "Test Accuracy:  0.9091851039190589\n",
      "Train accuracy:  0.8198467993379268\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score: \", f1)\n",
    "pred = np.dot(val_tx, w)\n",
    "pred[pred > 0.5] = 1\n",
    "pred[pred <= 0.5] = 0\n",
    "print(\"Test Accuracy: \", np.sum(pred == val_y) / len(val_y))\n",
    "pred = np.dot(train_tx, w)\n",
    "pred[pred > 0.5] = 1\n",
    "pred[pred <= 0.5] = 0\n",
    "print(\"Train accuracy: \", np.sum(pred == train_y) / len(train_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg logistic regression\n",
    "train_x_normalized = normalize_data(train_x)\n",
    "val_x_normalized = normalize_data(val_x)\n",
    "train_tx = build_model_data(train_x_normalized)\n",
    "val_tx = build_model_data(val_x_normalized)\n",
    "\n",
    "lambda_ = 0.001\n",
    "max_iters = 1000\n",
    "gamma = 2\n",
    "initial_w = np.full(train_tx.shape[1], -1)\n",
    "#initial_w = checkpoint2\n",
    "w, loss = reg_logistic_regression(train_y, train_tx, lambda_, initial_w, max_iters, gamma, False)\n",
    "f1 = compute_f1_logistic(val_y, val_tx, w)\n",
    "print(\"F1 score: \", f1)\n",
    "confusion_matrix(val_y, val_tx, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test():\n",
    "    path_test = \"/Users/robinfaro/Desktop/ML_EPFL/Project1/test_data_imputed.csv\"\n",
    "    test_data = np.genfromtxt(path_test, delimiter=\",\")\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test():\n",
    "    path_dataset = \"/Users/robinfaro/Desktop/ML_EPFL/Project1/dataset_to_release/x_test.csv\"\n",
    "    data = np.genfromtxt(path_dataset,\n",
    "                        delimiter=\",\",\n",
    "                        skip_header=1,\n",
    "                        usecols=[27, 31, 33, 38, 39, 40, 49, 51, 53, 61, 70, 88, 261, 266, 277, 278, 28, 29, 34, 35, 41, 43, 44, 45, 46, 47, 48,\n",
    "                                 66, 137, 145, 248, 1, 32, 255, 307, 67, 68, 69, 71, 72]) \n",
    "    \n",
    "    data[np.isnan(data)] = -1\n",
    "\n",
    "    data[:, 0][data[:, 0] > 5] = -1\n",
    "    data[:, 0][data[:, 0] >= 0] /= 5\n",
    "    data[:, 0][data[:, 0] == -1] = np.mean(data[:, 0][data[:, 0] != -1])\n",
    "\n",
    "    data[:, 1][data[:, 1] > 2] = -1\n",
    "    data[:, 1][data[:, 1] == 2] = 0\n",
    "    data[:, 0][data[:, 0] == -1] = mode(data[:, 0][data[:, 0] != -1])\n",
    "\n",
    "    data[:, 2][data[:, 2] > 2] = -1\n",
    "    data[:, 2][data[:, 2] == 2] = 0\n",
    "    data[:, 2][data[:, 2] == -1] = mode(data[:, 2][data[:, 2] != -1])\n",
    "\n",
    "    data[:, 3][data[:, 3] > 4] = -1\n",
    "    data[:, 3][data[:, 3] >= 0] /= 4\n",
    "    data[:, 3][data[:, 3] == -1] = np.mean(data[:, 3][data[:, 3] != -1])\n",
    "\n",
    "    data[:, 4][data[:, 4] > 2] = -1\n",
    "    data[:, 4][data[:, 4] == 2] = 0\n",
    "    data[:, 4][data[:, 4] == -1] = mode(data[:, 4][data[:, 4] != -1])\n",
    "\n",
    "    data[:, 5][data[:, 5] > 2] = -1\n",
    "    data[:, 5][data[:, 5] == 2] = 0\n",
    "    data[:, 5][data[:, 5] == -1] = mode(data[:, 5][data[:, 5] != -1])\n",
    "\n",
    "    data[:, 6][data[:, 6] > 4] = -1\n",
    "    data[:, 6][data[:, 6] == 2] = 0\n",
    "    data[:, 6][data[:, 6] == 3] = 0\n",
    "    data[:, 6][data[:, 6] == 4] = 2\n",
    "    data[:, 6][data[:, 6] >= 0] /= 2\n",
    "    data[:, 6][data[:, 6] == -1] = mode(data[:, 6][data[:, 6] != -1])\n",
    "\n",
    "    data[:, 7][data[:, 7] == 2] = 0\n",
    "\n",
    "    data[:, 8][data[:, 8] > 6] = -1\n",
    "    data[:, 8][data[:, 8] >= 0] /= 6\n",
    "    data[:, 8][data[:, 8] == -1] = np.mean(data[:, 8][data[:, 8] != -1])\n",
    "\n",
    "    data[:, 9][data[:, 9] > 8] = -1\n",
    "    data[:, 9][data[:, 9] >= 0] /= 8\n",
    "    data[:, 9][data[:, 9] == -1] = np.mean(data[:, 9][data[:, 9] != -1])\n",
    "\n",
    "    data[:, 10][data[:, 10] > 2] = -1\n",
    "    data[:, 10][data[:, 10] == 2] = 0\n",
    "    data[:, 10][data[:, 10] == -1] = mode(data[:, 10][data[:, 10] != -1])\n",
    "\n",
    "    data[:, 11][data[:, 11] > 2] = -1\n",
    "    data[:, 11][data[:, 11] == 2] = 0\n",
    "    data[:, 11][data[:, 11] == -1] = mode(data[:, 11][data[:, 11] != -1])\n",
    "\n",
    "    data[:, 12][data[:, 12] > 2] = -1\n",
    "    data[:, 12][data[:, 12] == 1] = 0\n",
    "    data[:, 12][data[:, 12] == 2] = 1\n",
    "    data[:, 12][data[:, 12] == -1] = mode(data[:, 12][data[:, 12] != -1])\n",
    "\n",
    "    data[:, 13][data[:, 13] > 2] = -1\n",
    "    data[:, 13][data[:, 13] == 1] = 0\n",
    "    data[:, 13][data[:, 13] == 2] = 1\n",
    "    data[:, 13][data[:, 13] == -1] = mode(data[:, 13][data[:, 13] != -1])\n",
    "\n",
    "    data[:, 14][data[:, 14] > 10] = 10\n",
    "    data[:, 14][data[:, 14] >= 0] /= 10\n",
    "    data[:, 14][data[:, 14] == -1] = np.mean(data[:, 14][data[:, 14] != -1])\n",
    "\n",
    "    data[:, 15][data[:, 15] > 10] = 10\n",
    "    data[:, 15][data[:, 15] >= 0] /= 10\n",
    "    data[:, 15][data[:, 15] == -1] = np.mean(data[:, 15][data[:, 15] != -1])\n",
    "\n",
    "    data[:, 16:18][data[:, 16:18] > 30] = -1\n",
    "    data[:, 16:18][data[:, 16:18] >= 0] /= 30\n",
    "    data[:, 16:18][data[:, 16:18] == -1] = np.mean(data[:, 16:18][data[:, 16:18] != -1])\n",
    "\n",
    "    data[:, 18][data[:, 18] > 4] = -1\n",
    "    data[:, 18][data[:, 18] >= 0] /= 4\n",
    "    data[:, 18][data[:, 18] == -1] = mode(data[:, 18][data[:, 18] != -1])\n",
    "\n",
    "    data[:, 19][data[:, 19] > 4] = -1\n",
    "    data[:, 19][data[:, 19] == 4] = 1\n",
    "    data[:, 19][data[:, 19] >= 2] = 0\n",
    "    data[:, 19][data[:, 19] == -1] = mode(data[:, 19][data[:, 19] != -1])\n",
    "\n",
    "    data[:, 20][data[:, 20] > 2] = -1\n",
    "    data[:, 20][data[:, 20] == 2] = 0\n",
    "    data[:, 20][data[:, 20] == -1] = mode(data[:, 20][data[:, 20] != -1])\n",
    "\n",
    "    data[:, 21][data[:, 21] > 2] = -1\n",
    "    data[:, 21][data[:, 21] == 2] = 0\n",
    "    data[:, 21][data[:, 21] == -1] = mode(data[:, 21][data[:, 21] != -1])\n",
    "\n",
    "    data[:, 22][data[:, 22] > 2] = -1\n",
    "    data[:, 22][data[:, 22] == 2] = 0\n",
    "    data[:, 22][data[:, 22] == -1] = mode(data[:, 22][data[:, 22] != -1])\n",
    "\n",
    "    data[:, 23][data[:, 23] > 2] = -1\n",
    "    data[:, 23][data[:, 23] == 2] = 0\n",
    "    data[:, 23][data[:, 23] == -1] = mode(data[:, 23][data[:, 23] != -1])\n",
    "\n",
    "    data[:, 24][data[:, 24] > 2] = -1\n",
    "    data[:, 24][data[:, 24] == 2] = 0\n",
    "    data[:, 24][data[:, 24] == -1] = mode(data[:, 24][data[:, 24] != -1])\n",
    "\n",
    "    data[:, 25][data[:, 25] > 2] = -1\n",
    "    data[:, 25][data[:, 25] == 2] = 0\n",
    "    data[:, 25][data[:, 25] == -1] = mode(data[:, 25][data[:, 25] != -1])\n",
    "\n",
    "    data[:, 26][data[:, 26] > 2] = -1\n",
    "    data[:, 26][data[:, 26] == 2] = 0\n",
    "    data[:, 26][data[:, 26] == -1] = mode(data[:, 26][data[:, 26] != -1])\n",
    "\n",
    "    data[:, 27][data[:, 27] > 2] = -1\n",
    "    data[:, 27][data[:, 27] == 2] = 0\n",
    "    data[:, 27][data[:, 27] == -1] = mode(data[:, 27][data[:, 27] != -1])\n",
    "\n",
    "    data[:, 28][data[:, 28] > 2] = -1\n",
    "    data[:, 28][data[:, 28] == 2] = 0\n",
    "    data[:, 28][data[:, 28] == -1] = mode(data[:, 28][data[:, 28] != -1])\n",
    "\n",
    "    data[:, 29][data[:, 29] > 2] = -1\n",
    "    data[:, 29][data[:, 29] == 2] = 0\n",
    "    data[:, 29][data[:, 29] == -1] = mode(data[:, 29][data[:, 29] != -1])\n",
    "\n",
    "    data[:, 30][data[:, 30] > 2] = -1\n",
    "    data[:, 30][data[:, 30] == 2] = 0\n",
    "    data[:, 30][data[:, 30] == -1] = mode(data[:, 30][data[:, 30] != -1])\n",
    "\n",
    "    #### ADDED COLUMNS ####\n",
    "\n",
    "    data[:, 31][data[:, 31] > 72] = -1\n",
    "    data[:, 31][data[:, 31] >= 0] /= 72\n",
    "    data[:, 31][data[:, 31] == -1] = np.mean(data[:, 31][data[:, 31] != -1])\n",
    "\n",
    "    data[:, 32][data[:, 32] > 3] = -1\n",
    "    data[:, 32][data[:, 32] == 3] = 0\n",
    "    data[:, 32][data[:, 32] == 2] = 1 \n",
    "    data[:, 32][data[:, 32] >= 0] /= 3\n",
    "\n",
    "    data[:, 33][data[:, 33] > 4] = -1\n",
    "    data[:, 33][data[:, 33] >= 0] /= 4\n",
    "    data[:, 33][data[:, 33] == -1] = np.mean(data[:, 33][data[:, 33] != -1])\n",
    "\n",
    "    data[:, 34][data[:, 34] > 2] = -1\n",
    "    data[:, 34][data[:, 34] == 2] = 0\n",
    "    data[:, 34][data[:, 34] == -1] = mode(data[:, 34][data[:, 34] != -1])\n",
    "\n",
    "    data[:, 35][data[:, 35] > 2] = -1\n",
    "    data[:, 35][data[:, 35] == 2] = 0\n",
    "    data[:, 35][data[:, 35] == -1] = mode(data[:, 35][data[:, 35] != -1])\n",
    "\n",
    "    data[:, 36][data[:, 36] > 2] = -1\n",
    "    data[:, 36][data[:, 36] == 2] = 0\n",
    "    data[:, 36][data[:, 36] == -1] = mode(data[:, 36][data[:, 36] != -1])\n",
    "\n",
    "    data[:, 37][data[:, 37] > 2] = -1\n",
    "    data[:, 37][data[:, 37] == 2] = 0\n",
    "    data[:, 37][data[:, 37] == -1] = mode(data[:, 37][data[:, 37] != -1])\n",
    "\n",
    "    data[:, 38][data[:, 38] > 2] = -1\n",
    "    data[:, 38][data[:, 38] == 2] = 0\n",
    "    data[:, 38][data[:, 38] == -1] = mode(data[:, 38][data[:, 38] != -1])\n",
    "\n",
    "    data[:, 39][data[:, 39] > 2] = -1\n",
    "    data[:, 39][data[:, 39] == 2] = 0\n",
    "    data[:, 39][data[:, 39] == -1] = mode(data[:, 39][data[:, 39] != -1])\n",
    "\n",
    "\n",
    "    return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = load_test()\n",
    "x_test_normalized = normalize_data(x_test)\n",
    "tx_test = build_model_data(x_test_normalized)\n",
    "y_test = sigmoid(tx_test.dot(checkpoint2))\n",
    "y_test[y_test >= 0.5] = 1\n",
    "y_test[y_test < 0.5] = -1\n",
    "ids = np.arange(328135, 437514)\n",
    "create_csv_submission(ids, y_test, \"submission_def.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "f1 = compute_f1_logistic(val_y, val_tx, checkpoint2, threshold)\n",
    "print(\"F1 score: \", f1)\n",
    "confusion_matrix(val_y, val_tx, checkpoint2, threshold)\n",
    "# val accuracy\n",
    "y_pred = sigmoid(val_tx.dot(checkpoint2))\n",
    "y_pred[y_pred >= threshold] = 1\n",
    "y_pred[y_pred < threshold] = 0\n",
    "print(\"Accuracy: \", np.mean(y_pred == val_y))\n",
    "# train accuracy\n",
    "y_pred = sigmoid(train_tx.dot(checkpoint2))\n",
    "y_pred[y_pred >= threshold] = 1\n",
    "y_pred[y_pred < threshold] = 0\n",
    "print(\"Accuracy: \", np.mean(y_pred == train_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
